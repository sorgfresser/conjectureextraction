from FlagEmbedding import BGEM3FlagModel
import torch
from typing import Optional
import gc


def format_doc_only_f(p: str) -> str:
    return f'''Formal Declaration: {p[:1536]}'''


model: Optional[BGEM3FlagModel] = None
device = torch.cuda.current_device() if torch.cuda.is_available() else torch.device('cpu')


def load_model():
    global model
    if model is None:
        model = BGEM3FlagModel("purewhite42/dependency_retriever_f", use_fp16=True)


def cleanup():
    global model
    model = None
    torch.cuda.empty_cache()
    gc.collect()


def get_similarities(formal_statements: list[list[str]], informal_statements: list[str]) -> list[torch.FloatTensor]:
    """
    Compute similarities between formal and informal statements.

    :param formal_statements: A list of lists, where the elements are formal_statements generated by multiple formalizers.
    :param informal_statements: A list of informal_statements.
    :return: A list of formalizers, each containing a similarity tensor.
    """
    sims = []
    with torch.no_grad():
        for formal in formal_statements:
            formal_embeddings = model.encode(
                formal,
                batch_size=64,
                max_length=1024)['dense_vecs']
            statement_embeddings = model.encode(
                informal_statements,
                batch_size=64,
                max_length=1024)['dense_vecs']
            similarities = (
                    torch.tensor(statement_embeddings).to(device=device, non_blocking=True).double() @ torch.tensor(
                formal_embeddings).to(device=device, non_blocking=True).double().T)
            sims.append(similarities.detach().cpu())
    return sims
